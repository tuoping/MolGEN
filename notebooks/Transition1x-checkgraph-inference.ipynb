{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31749741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    sim_ckpt=\"workdir/oa-TPS-x0std1.0-OT/L1/epoch=24-step=*.ckpt\",\n",
    "    data_dir=\"data/Transition1x/\",\n",
    "    suffix=\"\",\n",
    "    out_dir=\"./experiments/Transition1x/pretrained-x0std1.0-ot-frag_cutoffx1.5/ckpt.1.test_object-aware/\",\n",
    "    num_frames=3,\n",
    "    localmask=False,\n",
    "    tps_condition=True,\n",
    "    sim_condition=False\n",
    "    )\n",
    "import glob\n",
    "args.sim_ckpt = glob.glob(args.sim_ckpt)[0]\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525723df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, tqdm, time\n",
    "import numpy as np\n",
    "from mdgen.equivariant_wrapper import EquivariantMDGenWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75679b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "with open(f\"{args.out_dir}/README.md\", \"w\") as fp:\n",
    "    fp.write(args.sim_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69697478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mdgen.dataset import EquivariantTransformerDataset_Transition1x\n",
    "dataset = EquivariantTransformerDataset_Transition1x(data_dirname=args.data_dir, sim_condition=args.sim_condition, tps_condition=args.tps_condition, num_species=5, stage=\"test-fragmented_cutoffx1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a10fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(args.sim_ckpt, weights_only=False)\n",
    "hparams = ckpt[\"hyper_parameters\"]\n",
    "hparams['args'].guided = False\n",
    "# hparams['args'].sampling_method = 'euler'\n",
    "# hparams['args'].guidance_pref = 2\n",
    "hparams['args'].inference_steps = 50\n",
    "model = EquivariantMDGenWrapper(**hparams)\n",
    "print(model.model)\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f99428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt[\"hyper_parameters\"])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt[\"hyper_parameters\"]['args'].path_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt[\"hyper_parameters\"]['args'].x0std)\n",
    "print(ckpt[\"hyper_parameters\"]['args'].sampling_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d36015",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "sample_batch = next(iter(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[499][\"x\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6713b1",
   "metadata": {},
   "source": [
    "## Test generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d143595",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['species', 'x', 'cell', 'num_atoms', 'mask', 'v_mask', \"TKS_mask\", \"TKS_v_mask\", \"fragments_idx\"]:\n",
    "    try:\n",
    "        sample_batch[key] = sample_batch[key].to(device)\n",
    "    except:\n",
    "        print(f\"{key} not found\")\n",
    "\n",
    "\n",
    "pred_pos = model.inference(sample_batch)\n",
    "\n",
    "'''\n",
    "model.stage = \"inference\"\n",
    "prep = model.prep_batch(sample_batch)\n",
    "B,T,L,_ = prep[\"latents\"].shape\n",
    "t = torch.ones((B,), device=prep[\"latents\"].device)\n",
    "print(model.potential_model(prep['latents'], t, **prep['model_kwargs']).sum(dim=2).squeeze(-1)[:,1])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18163fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = model.prep_batch(sample_batch)\n",
    "print(prep['model_kwargs']['v_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea204d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rollout(model, batch):\n",
    "    expanded_batch = batch\n",
    "    \n",
    "    positions, _ = model.inference(expanded_batch)\n",
    "\n",
    "    # mask_act_space = (batch[\"mask\"] != 0)\n",
    "    # positions = positions*mask_act_space\n",
    "    new_batch = {**batch}\n",
    "    new_batch['x'] = positions\n",
    "    return positions, new_batch\n",
    "\n",
    "\n",
    "map_to_chemical_symbol = {\n",
    "    0: \"H\",\n",
    "    1: 'C',\n",
    "    2: \"N\",\n",
    "    3: \"O\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a24fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_rollouts = np.random.choice(len(dataset), size=300, replace=False)\n",
    "idx_rollouts = np.arange(len(dataset))\n",
    "# idx_rollouts = np.arange(300)\n",
    "print(idx_rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdd6cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd14062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.geometry.geometry import get_distances\n",
    "import shutil, os\n",
    "from ase.io import write\n",
    "\n",
    "all_rollout_atoms_ref_0 = []\n",
    "all_rollout_atoms = []\n",
    "all_rollout_atoms_ref = []\n",
    "start = time.time()\n",
    "\n",
    "for i_rollout in range(len(idx_rollouts)):\n",
    "    if 'Ensemble' in args.out_dir:\n",
    "        idx = idx_rollouts[0]\n",
    "    else:\n",
    "        idx = idx_rollouts[i_rollout]\n",
    "# for i_rollout in range(len(idx_rollouts)):\n",
    "#     idx = idx_rollouts[i_rollout]\n",
    "    item = dataset.__getitem__(idx)\n",
    "    batch = next(iter(torch.utils.data.DataLoader([item])))\n",
    "\n",
    "    for key in ['species', 'x', 'cell', 'num_atoms', 'mask', 'v_mask', \"TKS_mask\", \"TKS_v_mask\", \"fragments_idx\"]:\n",
    "        try:\n",
    "            batch[key] = batch[key].to(device)\n",
    "        except:\n",
    "            print(f\"{key} not found\")\n",
    "\n",
    "    labels = torch.argmax(batch[\"species\"], dim=3).squeeze(0)\n",
    "    symbols = [[map_to_chemical_symbol[int(i_elem.to('cpu'))] for i_elem in labels[i_conf]] for i_conf in range(len(labels))]\n",
    "\n",
    "    pred_pos, _ = rollout(model, batch)\n",
    "    # print(\"idx = \", idx, \"rollout\", i_rollout, pred_pos.shape)\n",
    "\n",
    "    all_atoms = []\n",
    "    all_atoms_ref = []\n",
    "    all_atoms_ref_0 = []\n",
    "    for t in range(len(pred_pos[0])):\n",
    "        print(\"rollout\", i_rollout, \"idx = \", idx, \"t\", t)\n",
    "        formula = \"\".join(symbols[t])\n",
    "\n",
    "        atoms = Atoms(formula, positions=pred_pos[0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "        # atoms.set_chemical_symbols(symbols[t])\n",
    "        all_atoms.append(atoms)\n",
    "        if args.sim_condition:\n",
    "            atoms_ref_0 = Atoms(formula, positions=batch[\"x\"][0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "            atoms_ref = Atoms(formula, positions=batch[\"x_next\"][0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "        else:\n",
    "            atoms_ref = Atoms(formula, positions=batch[\"x\"][0][t].cpu().numpy(), cell=batch['cell'][0][0].cpu().numpy(), pbc=[1,1,1])\n",
    "        all_atoms_ref.append(atoms_ref)\n",
    "        if args.sim_condition:\n",
    "            all_atoms_ref_0.append(atoms_ref_0)\n",
    "        if args.tps_condition:\n",
    "            if t == 1:\n",
    "                err = (pred_pos[0][t]-batch[\"x\"][0][t]).norm(dim=-1)\n",
    "                print(err.max(), err.min(), torch.sqrt((err**2).mean(dim=-1)), )\n",
    "            #     assert not torch.allclose(pred_pos[0][t], batch[\"x\"][0][t])\n",
    "            #     assert not np.allclose(pred_pos[0][t].cpu().numpy(), batch[\"x\"][0][t].cpu().numpy())\n",
    "            # else:\n",
    "            #     assert torch.allclose(pred_pos[0][t], batch[\"x\"][0][t])\n",
    "    # all_rollout_atoms.append(all_atoms)\n",
    "    # all_rollout_atoms_ref.append(all_atoms_ref)\n",
    "    # if args.sim_condition:\n",
    "    #     all_rollout_atoms_ref_0.append(all_atoms_ref_0)\n",
    "    out_dir = args.out_dir\n",
    "    dirname = os.path.join(out_dir, f\"rollout_{i_rollout}\")\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    with open(os.path.join(dirname, \"README.md\"), \"w\") as fp:\n",
    "        fp.write(\"Data index from Transition1x: %d\"%idx)\n",
    "    np.savetxt(os.path.join(dirname, \"Fragment_idx.dat\"), prep['model_kwargs']['fragments_idx'].squeeze(0).cpu().numpy())\n",
    "    filename = os.path.join(dirname, \"gentraj_1.xyz\")\n",
    "    filename_ref = os.path.join(dirname, \"reftraj_1.xyz\")\n",
    "    print(filename_ref)\n",
    "    if os.path.exists(filename):\n",
    "    #     shutil.move(filename_0, os.path.join(dirname, \"bck.0.gentraj_0.xyz\"))\n",
    "        os.remove(filename)\n",
    "    #     shutil.move(filename_ref_0, os.path.join(dirname, \"bck.0.reftraj_0.xyz\"))\n",
    "        os.remove(filename_ref)\n",
    "    assert not np.allclose(all_atoms[1].positions, all_atoms_ref[1].positions)\n",
    "    for atoms in all_atoms:\n",
    "        atoms.set_cell(np.eye(3,3)*25)\n",
    "        write(filename, atoms, append=True)\n",
    "    for ref_atoms in all_atoms_ref:\n",
    "        ref_atoms.set_cell(np.eye(3,3)*25)\n",
    "        write(filename_ref, ref_atoms, append=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "for i_rollout in range(10):\n",
    "    print(\"rollout\", i_rollout, pred_pos.shape)\n",
    "    all_atoms = all_rollout_atoms[i_rollout]\n",
    "    all_atoms_ref = all_rollout_atoms_ref[i_rollout]\n",
    "    for t in range(len(pred_pos[0])):\n",
    "        print(\"t=\",t)\n",
    "        atoms = all_atoms[t]\n",
    "        atoms_ref = all_atoms_ref[t]\n",
    "        for i in range(atoms.positions.shape[0]):\n",
    "            err = get_distances(atoms_ref.positions[i], atoms.positions[i], cell=atoms.cell, pbc=True)[1][0][0]\n",
    "\n",
    "            if err>0.1:\n",
    "                print(atoms.positions[i], atoms_ref.positions[i], err, err>0.1)\n",
    "        \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odefed_mdgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
